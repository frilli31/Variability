% Use only LaTeX2e, calling the article.cls class and 12-point type.

\documentclass[12pt]{article}

% Users of the {thebibliography} environment or BibTeX should use the
% scicite.sty package, downloadable from *Science* at
% www.sciencemag.org/about/authors/prep/TeX_help/ .
% This package should properly format in-text
% reference calls and reference-list numbers.

\usepackage{scicite}

% Use times if you have the font installed; otherwise, comment out the
% following line.

%\usepackage{times}

% The preamble here sets up a lot of new/revised commands and
% environments.  It's annoying, but please do *not* try to strip these
% out into a separate .sty file (which could lead to the loss of some
% information when we convert the file to other formats).  Instead, keep
% them in the preamble of your main LaTeX source file.


% The following parameters seem to provide a reasonable page setup.

\topmargin 0.0cm
\oddsidemargin 0.2cm
\textwidth 16cm 
\textheight 21cm
\footskip 1.0cm


%The next command sets up an environment for the abstract to your paper.

\newenvironment{sciabstract}{%
\begin{quote} \bf}
{\end{quote}}


% If your reference list includes text notes as well as references,
% include the following line; otherwise, comment it out.

\renewcommand\refname{References and Notes}

% The following lines set up an environment for the last note in the
% reference list, which commonly includes acknowledgments of funding,
% help, etc.  It's intended for users of BibTeX or the {thebibliography}
% environment.  Users who are hand-coding their references at the end
% using a list environment such as {enumerate} can simply add another
% item at the end, and it will be numbered automatically.

\newcounter{lastnote}
\newenvironment{scilastnote}{%
\setcounter{lastnote}{\value{enumiv}}%
\addtocounter{lastnote}{+1}%
\begin{list}%
{\arabic{lastnote}.}
{\setlength{\leftmargin}{.22in}}
{\setlength{\labelsep}{.5em}}}
{\end{list}}


% Include your paper's title here

\title{
		\includegraphics[scale=0.20]{logo-unipd}~ 
		\\[2cm]
		Report of {\it Process Mining\/} Project
	} 


% Place the author information here.  Please hand-code the contact
% information and notecalls; do *not* use \footnote commands.  Let the
% author contact information appear immediately below the author names
% as shown.  We would also prefer that you don't change the type-size
% settings shown here.

\author
{Luca Allegro 1211142 \\ Alberto Bezzon 1211016\\
\\
Department of Mathematics (Computer Science), University of Padua
}

% Include the date command, but leave its argument blank.

\date{}



%%%%%%%%%%%%%%%%% END OF PREAMBLE %%%%%%%%%%%%%%%%

% My packages
\usepackage{graphicx}
\usepackage[newfloat]{minted}
\usepackage{caption}
\usepackage{placeins}
\usepackage{float}
\usepackage{nameref}
\usepackage{hyperref}

\graphicspath{ {./images/} } 
%\usemintedstyle[python]{monokai}
\newenvironment{code}{\captionsetup{type=listing}}{}
\SetupFloatingEnvironment{listing}{name=Code snippet}

\hypersetup{
	colorlinks = true,
	linkcolor = {blue},
	citecolor = {red},
	urlcolor = {blue},
}
\begin{document} 

% Double-space the manuscript.

\baselineskip18pt

% Make the title.

\maketitle 



% Place your abstract within the special {sciabstract} environment.

\begin{sciabstract}
	The purpose of this report is to present the \textit{Project 1: How much variable is my event log?} carried out for the Process Mining course at University of Padua.
\end{sciabstract}



% In setting up this template for *Science* papers, we've used both
% the \section* command and the \paragraph* command for topical
% divisions.  Which you use will of course depend on the type of paper
% you're writing.  Review Articles tend to have displayed headings, for
% which \section* is more appropriate; Research Articles, when they have
% formal topical divisions at all, tend to signal them with bold text
% that runs into the paragraph, for which \paragraph* is the right
% choice.  Either way, use the asterisk (*) modifier, as shown, to
% suppress numbering.

\newpage
\section*{Introduction}

The goal of this report is to sum up the Process Mining project. In this project we aim at measuring how much variable are event logs, in terms of variety of behaviour. Computing the variability of a log can indeed be rather useful, for instance, to decide for (a procedural or a declarative) model discovery, or to decide which prediction technique to apply. The are a lot of different ways to measure the variability of an event log, three of them are described in the following sections.

\smallskip
A possible way to measure the variability of an event log is counting the number of variants that it contains. A second possibility is to average the edit distance between each pair of traces in the event log. 

\smallskip
The report is organized as follows: in Sections \nameref{section:first}, \nameref{section:second} and \nameref{section:third} the three ways to compute variability of a log have been proposed. For each section there are two subsections that explain advantages and disadvantages of the metric described in the section. Section \nameref{section:results} contains the results performed by executing the functions in the BPIChallenge2011 log. The last section \nameref{section:structure} describes the project structure and provides instructions for executing the proposed code and related tests.


\section*{First approach}\label{section:first}

The first and easiest way to measure variability of an event log is counting the number of variants that it contains. The function in \hyperref[code:code1]{Snippet 1} computes this metric. We recall the definition of event, trace, process variant and process log: an event is an occurrence of an activity in a particular process instance, a trace is a sequence of events of the same such instance, a process variant is a sequence of process activities and a log is a multiset of such traces.

\begin{code}
	\captionof{listing}{Function compute\_variant\_variability}
	\label{code:code1}
	\begin{minted}[
	frame=lines,
	framesep=2mm,
	baselinestretch=1.2,
	fontsize=\footnotesize,
	linenos
	]{python}
def compute_variant_variability(log: lg.EventLog) -> int:
"""Compute the number of variants present in the event log

Args:
log (lg.EventLog): The log to examine

Returns:
int: The number of variants present in the event log
"""
# For each case of the log we construct a tuple with the names of events.
# Then we collect all of them in a set to remove duplicates.
# Eventually we compute the length of the set.
return len(set(tuple(event["concept:name"] for event in case) for case in log))
	\end{minted}
\end{code}


\noindent This is the simplest and intuitive way to compute log variability. However, this metric does not consider the size of the log, i.e. big size log are penalized with respect to small size log. There is also a second reason that this way is unhelpful to our goal: it does not consider how variants differ to one another.

\paragraph*{Example:} Consider the following two logs L\textsubscript{1}=[\textless a,b,c\textgreater, \textless a,b,d\textgreater] and L\textsubscript{2}=[\textless a,b,c\textgreater, \textless b,c\textgreater, \textless x,y,z\textgreater]. The function returns 2 in both cases, but L\textsubscript{1} has more similar variants rather than L\textsubscript{2} which variants are completely different. This example shows how poorly informative this metric is.


\section*{Second approach}\label{section:second}

In order to obtain a more informative metric to compute log variability, the second approach that we developed consists in averaging edit distance between each pair of traces in the event log. For this purpose, we decided to use the Levenshtein distance\cite{WEBSITE:1} that is the most well-known string edit distance metric and it is defined by the number of insertions, deletions and substitutions required to convert one string into another. The basic idea to compute average edit distance is, first compute the sum of edit distance of all possible combination of two pair of traces and then divide it by the number of possible combination that is \texttt{size\_of\_log $\cdot$ (size\_of\_log - 1)}.

\begin{code}
	\captionof{listing}{Function compute\_edit\_distance\_variability}
	\label{code:code2}
	\begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
linenos
]{python}
def compute_edit_distance_variability(log: lg.EventLog) -> float:
"""Compute the average edit distance (Levenshtein distance) between each
pairs of traces.

This function uses function `eval` of module 'editdistance' because it's
implemented in C++ and it is faster than the corresponding implementation
in python.

Args:
log (EventLog): The log to examine

Returns:
float: the average edit distance between each pairs of traces
"""
# Create a dictionary which contains variants as keys and its number of
# occurrences as values
variants_and_counts = Counter(
tuple(event["concept:name"] for event in case) for case in log
)
size_of_log = len(log)

# For each pair of distinct variants (obtained by 'combinations') we
# compute the edit distance and we multiply it for number of
# occurrences of the variants.
# In the end we sum all of them.
sum_of_distances = sum(
num_of_items_1 * num_of_items_2 * editdistance.eval(variant1, variant2)
for (variant1, num_of_items_1), (variant2, num_of_items_2)
in combinations(variants_and_counts.items(), 2)
)

# We multiply the sum of distances by 2 because to add the sum of distances
# of each inverted pairs of variants.
# In the end we divide it by the number of possible combination of pair
# of traces
return float(sum_of_distances * 2) / (size_of_log * (size_of_log - 1))
	\end{minted}
\end{code}

\noindent Some important caveats are:
\begin{itemize}
	\item we use variants as an alternative to traces because, instead of calculating edit distance between two traces \textit{n} times, we calculate the edit distance of two different traces multiplying by the number of repetitions of a trace (\texttt{line 26} of \hyperref[code:code2]{Snippet 2}).
	\item \texttt{combination} method of the python library \texttt{itertools} provides all the combinations of length two of different variants (\texttt{line 29} of \hyperref[code:code2]{Snippet 2}); the only case not covered by combinations is when two traces have the same variant but the edit distance is 0, so it does not affect the result (because the sum of all distances does not change summing 0).
	\item To optimize the calculation of edit distance, we use a C++ library with API for python because the above function has high complexity and with very big log (e.g. \textit{BPIchallenge2011.xes}) it takes very long time\footnote{We know that computing edit distance between two traces has complexity O(\textit{n$\cdot$m}) where \textit{n} and \textit{m} are the lengths of the traces. The whole function has a complexity O(\textit{l}\textsuperscript{2}$\cdot$\textit{n$\cdot$m}) where \textit{l} is the number of variants in the event log and \textit{n} and \textit{m} are the lengths of the longest traces.}.
\end{itemize}

\subsection*{Advantages}

\noindent The main advantage is that this metric solves the problems of the metric described in Section \hyperref[section:first]{First approach}. In fact:
\begin{itemize}
	\item it takes into account the size of the log. Unlike the previous metric, this does not penalize big size logs.
	\item it considers how variants differ one another.
\end{itemize}

\subsection*{Disadvantages}

This method has also the following disadvantages:
\begin{itemize}
	\item it is very time-consuming for big size logs or long traces due to the fact that it has a high complexity.
	\item it does not consider the length of each trace, that's why logs with long traces are penalized).\label{ref:cons2}
\end{itemize}  

\paragraph*{Example:} Consider the following logs: L\textsubscript{1}=[\textless a,b\textgreater, \textless c,d\textgreater] and L\textsubscript{2}=[\textless a,b,c,d,e\textgreater,\\ \textless a,b,c,f,g\textgreater]. The function \texttt{compute\_edit\_distance\_variability} returns 2 for both logs but L\textsubscript{1} has a higher variability compared to L\textsubscript{2} that has the first three events equal in both traces (the traces in L\textsubscript{2} are more similar). 
\newpage
\section*{Third approach}\label{section:third}

To address the weakness of the metrics based on edit distance, we developed a third way that consists in normalization of edit distance (we found this approach in Section \textit{3.6.1 Edit Distance Between Traces} of article \cite{ARTICLE:1}).  We normalise edit distance by the greatest possible distance between the traces to reflect that a distance of one operation on two very long strings should be considered less significant than on very short strings. In this way, we are able to compare the result with logs with a different average length.
\begin{code}
	\captionof{listing}{Function compute\_my\_variability}
	\label{code:code3}
	\begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
linenos
]{python}
def compute_my_variability(log: lg.EventLog) -> float:
"""Compute the average of normalized edit distances (Levenshtein distance)
between each pairs of traces.

This function uses function `eval` of module 'editdistance' because it's
implemented in C++ and it is faster than the corresponding implementation
in python.

Args:
	log (EventLog): The log to examine

Returns:
	float: a number between 0 and 1 included. The higher the number the
	more similar the traces are
		- 0 : if all traces has nothing in common
		- 1 : all traces belongs to the same variant (are equals)
"""
# Create a dictionary which contains variants as keys and its number of
# occurrences as values
variants_and_counts = Counter(
			tuple(event["concept:name"] for event in case) for case in log
			)
size_of_log = len(log)

# For each pair of distinct variants (obtained by 'combinations') we
# compute the average edit distance normalized (divided by longest trace)
# and we multiply it for number of occurrences of the variants.
# In the end we sum all of them.
sum_of_distances = sum(
	float(num_of_items_1 * num_of_items_2 * editdistance.eval(variant1, variant2))
	/ max(len(variant1), len(variant2))
	for (variant1, num_of_items_1), (variant2, num_of_items_2)
	in combinations(variants_and_counts.items(), 2)
)

# We multiply the sum of distances by 2 because to add the sum of distances
# of each inverted pairs of variants
# In the end we divide it by the number of possible combination of pair
# of traces
return 1 - (sum_of_distances * 2 / (size_of_log * (size_of_log - 1)))
	\end{minted}
\end{code}

\subsection*{Advantages}

This metric produces a result in the range [0, 1], that is immediately interpretable because it represent the fraction of how much traces are equals. In fact, the higher the number is, the
more similar the traces are. This number is 0 if all traces of the log have nothing in common. Conversely, it is 1 when all traces of the log have the same variant. In conclusion, this metric solve the \hyperref[ref:cons2]{second disadvantage} of the previous metric.

\paragraph*{Example:} Consider once again the two log of the previous example (L\textsubscript{1}=[\textless a,b\textgreater, \textless c,d\textgreater] and L\textsubscript{2}=[\textless a,b,c,d,e\textgreater, \textless a,b,c,f,g\textgreater] reported for convenience). The function in \hyperref[code:code3]{Snippet 3} returns 0 for L\textsubscript{1} which traces are completely different and 0.6 for L\textsubscript{2} which traces are somehow similar.

\subsection*{Disadvantages}

The main disadvantage of this metric is that it does not consider cycles in traces, i.e. traces with a repeated number of event are wrongly penalized. See the following example.

\paragraph*{Example:} Consider the following two logs: L\textsubscript{1}=[\textless a,b,c,b,c,b,c,b,c,b,c,b,c\textgreater, \textless a,b,c,b,c\textgreater] and L\textsubscript{2}=[\textless a,b,c,p,q,r\textgreater, \textless a,b,c,d,e,f\textgreater]. The function \texttt{compute\_my\_variability} returns 0.38 for L\textsubscript{1} and 0.5 for L\textsubscript{2}; however L\textsubscript{1} is less variable w.r.t. L\textsubscript{2} but it si penalize because the first trace in L\textsubscript{2} is a very long traces.

\section*{BPI Challenge 2011}\label{section:results}

In this section, we report and discuss the results of the three different metrics realized. The following table summarize the results.
\renewcommand{\arraystretch}{1.5}\\
\begin{table}[H]
	\centering
		\begin{tabular}{ |c|c|  }
			\hline
			\textbf{Functions} & \textbf{Results }\\
			\hline
			\texttt{compute\_variant\_variability} & 981 \\ \hline
			\texttt{compute\_edit\_distance\_variability} & 195.88194492325937 \\ \hline
			\texttt{compute\_my\_variability} & 0.14258102346211654 \\
			\hline
		\end{tabular}
	\caption{Table summarizing the results obtained}
	\label{table:results}
\end{table}

As you can see in \hyperref[table:results]{Table 1}, the result obtained applying \texttt{compute\_variant\_variability} is 981. This number indicates that BPIChallenge2011 log has 981 of variants. We verified this data importing the log on Disco and the two number match. The only conclusion, given the size of log (1143), is that most of traces are different one another. Thus, this metric is too simple too capture an interesting variability that involves how different are this variants.

\smallskip  

The second result obtained applying \texttt{compute\_edit\_distance\_variability} is $\approx$ \textit{195.88}. This number is more informative w.r.t the previous result and it tell us that there is a big average edit distance between pair of traces, hence, the log has an high variability. But you cannot understand how much traces have in common.

\smallskip

The last result obtained by applying \texttt{compute\_my\_variability} is $\approx$ \textit{0.14}. This number gives more information than the others, because it tells how much two traces, random picked, have in common. In this case, the result is 15\% which means that traces are different and we can conclude that the log has high variability.
\newpage
\section*{Project structure}\label{section:structure}

The project is structured in the following manner:
\begin{itemize}
	\item \texttt{\_init.py}: contains the functions that implements the three different metrics
	\item \texttt{\_test.py}: contains the test cases
	\item \texttt{main.py}: contains an example of using the three functions
	\item directory \texttt{resources} contains examples of logs used for testing functions
		
\end{itemize}

\subsubsection*{Instructions}

The following steps are the instructions to run the code.
\begin{enumerate}
	\item Extract \textit{Progetto.zip}
	\item Open a terminal inside the folder
	\item Install requirements using the following command: \texttt{pip install requirements.txt}
	\item Run the tests using: \texttt{pytest}
	\item In file \texttt{main.py} you can find an use example and you can run it: \texttt{python main.py}
\end{enumerate}

\bibliography{scifile} 
\bibliographystyle{ieeetr}

\end{document}




















